{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Libraries and Data Base</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from IPython import display\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix\n",
    "from __future__ import division\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Labeled Data base\n",
    "filename = 'DataBases/LBPTruncadoRaw4.csv'\n",
    "csv = np.genfromtxt (filename, delimiter=',')\n",
    "Y = csv[:,0]\n",
    "X = csv[:,1:]\n",
    "print \"Labels:\"\n",
    "print Y.shape\n",
    "print \"Data:\"\n",
    "print X.shape\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Dimensionality Reduction</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Data normalization\n",
    "\n",
    "#data_scaled = scale(X, axis=0, with_mean=False, with_std=True)\n",
    "#print \"Datos Normalizados\"\n",
    "#print data_scaled.shape\n",
    "\n",
    "#PCA projection\n",
    "#pca = PCA(n_components=500)\n",
    "#X_new = pca.fit_transform(data_scaled)\n",
    "\n",
    "print \"Data:\"\n",
    "print X.shape\n",
    "\n",
    "#Dimensionality Reduction with Random Forest model\n",
    "clf = RandomForestClassifier()\n",
    "clf = clf.fit(X, Y)\n",
    "model = SelectFromModel(clf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "print \"New data:\"\n",
    "print X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data base partition</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Decide wheter reduce dimensionality (if yes, run the previous cell beforehand)\n",
    "dimRed = True\n",
    "\n",
    "#size of datasets\n",
    "trainSize = 0.6\n",
    "valiSize = 0.1\n",
    "#the test size is not given because it is fixed with the previous two fractions\n",
    "\n",
    "if dimRed:\n",
    "    data = X_new\n",
    "else:\n",
    "    data = X\n",
    "\n",
    "#Split\n",
    "X_train, X_aux, Y_train, Y_aux = train_test_split(data[350:,:], Y[350:], train_size=trainSize) #balanced\n",
    "#X_train, X_aux, Y_train, Y_aux = train_test_split(data, Y, train_size=trainSize) #original\n",
    "X_vali, X_test, Y_vali, Y_test = train_test_split(X_aux, Y_aux, train_size= valiSize/(1-trainSize))\n",
    "\n",
    "print \"Training set:\"\n",
    "print \"Labels:\"\n",
    "print Y_train.shape\n",
    "print \"Data:\"\n",
    "print X_train.shape\n",
    "men = 0\n",
    "women = 0\n",
    "for i in range(len(Y_train)):\n",
    "    if (Y_train[i]==1):\n",
    "        women = women + 1\n",
    "    else:\n",
    "        men = men + 1\n",
    "print \"Men in training set:\"\n",
    "print men\n",
    "print \"Women in training set:\"\n",
    "print women              \n",
    "\n",
    "print \"\"\n",
    "print \"Validation set:\"\n",
    "print \"Labels:\"\n",
    "print Y_vali.shape\n",
    "print \"Data:\"\n",
    "print X_vali.shape\n",
    "men = 0\n",
    "women = 0\n",
    "for i in range(len(Y_vali)):\n",
    "    if (Y_vali[i]==1):\n",
    "        women = women + 1\n",
    "    else:\n",
    "        men = men + 1\n",
    "print \"Men in training set:\"\n",
    "print men\n",
    "print \"Women in training set:\"\n",
    "print women  \n",
    "\n",
    "print \"\"\n",
    "print \"Test set:\"\n",
    "print \"Labels:\"\n",
    "print Y_test.shape\n",
    "print \"Data:\"\n",
    "print X_test.shape\n",
    "men = 0\n",
    "women = 0\n",
    "for i in range(len(Y_test)):\n",
    "    if (Y_test[i]==1):\n",
    "        women = women + 1\n",
    "    else:\n",
    "        men = men + 1\n",
    "print \"Men in training set:\"\n",
    "print men\n",
    "print \"Women in training set:\"\n",
    "print women  \n",
    "\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>LDA Classifier</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#LDA with the entire dataset\n",
    "classifier = LinearDiscriminantAnalysis()\n",
    "classifier.fit(X, Y)\n",
    "data_projected = classifier.transform(X)\n",
    "\n",
    "#Plotting the result\n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(range(len(X)) , data_projected, c=Y, linewidth=0, alpha=0.5, s=20)\n",
    "plt.xlim([0,len(X)-1])\n",
    "plt.xlabel(\"Data Instance\")\n",
    "plt.ylabel(\"LDA Projection\")\n",
    "plt.grid()\n",
    "plt.title('LDA Projection, Entire Set')\n",
    "\n",
    "\n",
    "\n",
    "#Test the LDA as a classifier\n",
    "#For this, is important to train the LDA correctly, that is, with the X_train dataset only.\n",
    "\n",
    "#print 'Training results:'\n",
    "#predicted_training_labels = classifier.predict_proba(X_train)[:,1]\n",
    "#[[VN,FP],[FN,VP]]=confusion_matrix(Y_train.astype(bool),(predicted_training_labels>0.5).astype(bool)).astype(float)\n",
    "#print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "#print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "#print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "#print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "#print ''\n",
    "\n",
    "#print 'Test results:'\n",
    "#predicted_test_labels = classifier.predict_proba(X_test)[:,1]\n",
    "#[[VN,FP],[FN,VP]]=confusion_matrix(Y_test.astype(bool),(predicted_test_labels>0.5).astype(bool)).astype(float)\n",
    "#print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "#print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "#print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "#print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "#print ''\n",
    "\n",
    "#fpr, tpr, th = roc_curve(Y_test, predicted_test_labels)\n",
    "    \n",
    "#plt.figure(figsize=(10, 4))\n",
    "#plt.subplot2grid((1,2),(0,0))\n",
    "#plt.plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "#plt.xlabel(\"False Positive Rate\")\n",
    "#plt.ylabel(\"True Positive Rate\")\n",
    "#plt.grid()\n",
    "#plt.title('ROC Curve')\n",
    "\n",
    "#plt.subplot2grid((1,2),(0,1))\n",
    "#plt.plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "#plt.xlabel(\"False Positive Rate\")\n",
    "#plt.ylabel(\"False Negative Rate\")\n",
    "#plt.yscale('log')\n",
    "#plt.xscale('log')\n",
    "#plt.grid()\n",
    "#plt.title('DET Curve')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Two Layer MLP Configuration</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#-----------------------\n",
    "#PARAMETERS OF INICIALIZATION\n",
    "#-----------------------\n",
    "\n",
    "# Training parameters\n",
    "minibatch_size = 8\n",
    "n_epochs = 2000\n",
    "n_training_samples = len(Y_train)\n",
    "target_cost = 0.01\n",
    "\n",
    "#Gradient Descent parameters\n",
    "learning_rate = 0.1\n",
    "momentum = 0.1\n",
    "\n",
    "#Number of neurons\n",
    "n_inputs = len(X_train[0])\n",
    "n_hidden = 15\n",
    "n_classes = 2\n",
    "n_neurons = np.hstack((n_inputs,n_hidden,n_classes)).astype(int)\n",
    "\n",
    "print \"Number of inputs:\"\n",
    "print n_inputs\n",
    "\n",
    "#-----------------------\n",
    "#CONSTRUCTION OF THE MLP\n",
    "#-----------------------\n",
    "\n",
    "MLP_input = tf.placeholder(tf.float32, shape=[None, n_inputs])\n",
    "previous_layer = MLP_input\n",
    "\n",
    "# Pointer to weights and biases for regularization\n",
    "regularizers = 0\n",
    "\n",
    "# Iteration constructing one layer at a time\n",
    "for level in range(len(n_neurons)-1):\n",
    "    weights = tf.Variable(tf.random_uniform   ([n_neurons[level],n_neurons[level+1]], minval= -np.sqrt(3/n_neurons[level]),maxval=np.sqrt(3/n_neurons[level])))\n",
    "    #weights = tf.Variable(tf.truncated_normal([n_neurons[level],n_neurons[level+1]], stddev=0.1))\n",
    "    biases = tf.Variable(tf.constant(0., shape=[n_neurons[level+1]]))\n",
    "    applied_weights = tf.matmul(previous_layer, weights) + biases\n",
    "\n",
    "    regularizers += tf.nn.l2_loss(weights) + tf.nn.l2_loss(biases)\n",
    "\n",
    "    if level < len(n_neurons)-2:\n",
    "        layer = tf.sigmoid(applied_weights)\n",
    "        previous_layer = layer\n",
    "    else:\n",
    "        MLP = tf.nn.softmax(applied_weights)\n",
    "        break\n",
    "        \n",
    "#-----------------------\n",
    "#COST FUNCTION\n",
    "#-----------------------\n",
    "\n",
    "# Accomodate target\n",
    "target = tf.placeholder(tf.float32, shape=[None])\n",
    "one_hot_target = tf.one_hot(tf.cast(target, dtype=tf.int32), 2)\n",
    "\n",
    "# Cross Entropy\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(one_hot_target * tf.log(tf.clip_by_value(MLP,1e-10,1.0)), reduction_indices=[1]))\n",
    "\n",
    "# Mean Squared Error\n",
    "mean_squared_error = tf.sqrt(tf.reduce_mean(tf.square(target - MLP[:,1])))\n",
    "\n",
    "\n",
    "#cost_function = mean_squared_error\n",
    "cost_function = cross_entropy\n",
    "\n",
    "#-----------------------\n",
    "#REGULARIZATION\n",
    "#-----------------------\n",
    "#cost_function += 0.0005 * regularizers\n",
    "\n",
    "#-----------------------\n",
    "#OPTIMIZATION METHOD\n",
    "#-----------------------\n",
    "\n",
    "# Gradient Descent with Momentum\n",
    "GDM_optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "\n",
    "# Adam Algorithm\n",
    "Adam_optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "#training_algorithm = GDM_optimizer.minimize(cost_function)\n",
    "training_algorithm = Adam_optimizer.minimize(cost_function)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLP Training</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "n_minibatches = int(np.shape(X_train)[0] / minibatch_size) \n",
    "\n",
    "cost_valid_history = []\n",
    "cost_train_history = []\n",
    "\n",
    "prev_cost_valid = 10.0\n",
    "validation_checks = 0\n",
    "max_validation_checks = 15\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i_mb in range(n_minibatches):\n",
    "        a,b = i_mb*minibatch_size, (i_mb+1)*minibatch_size\n",
    "        X_minibatch = X_train[a:b,:]\n",
    "        Y_minibatch = Y_train[a:b]\n",
    "        sess.run(training_algorithm,feed_dict={MLP_input: X_minibatch, target: Y_minibatch})\n",
    "        \n",
    "    cost_valid = sess.run(cost_function,feed_dict={MLP_input: X_vali, target: Y_vali})\n",
    "    cost_valid_history += [cost_valid]\n",
    "    \n",
    "    cost_train = sess.run(cost_function,feed_dict={MLP_input: X_train, target: Y_train})\n",
    "    cost_train_history += [cost_train]\n",
    "    \n",
    "    if prev_cost_valid < cost_valid:\n",
    "        validation_checks += 1\n",
    "    else:\n",
    "        validation_checks = 0\n",
    "        prev_cost_valid = cost_valid\n",
    "        \n",
    "    print(\"Epoch: %d/%d, Training cost: %f, Validation cost: %f, Validation checks: %d/%d\" %(epoch+1, n_epochs,cost_train,cost_valid,validation_checks,max_validation_checks))\n",
    "    \n",
    "    if cost_train <= target_cost:\n",
    "        print 'Target cost reached'\n",
    "        break\n",
    "    if validation_checks >= max_validation_checks:\n",
    "        print 'Early stopping'\n",
    "        break\n",
    "    \n",
    "    \n",
    "print ''\n",
    "    \n",
    "fig = plt.figure(figsize=(6, 4))\n",
    "plt.plot(cost_valid_history, label=\"Validation\")\n",
    "plt.plot(cost_train_history, label=\"Training\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cost history\")\n",
    "plt.grid()\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>MLP Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print 'Training results:'\n",
    "predicted_training_labels = sess.run(MLP[:,1],feed_dict={MLP_input: X_train})\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(Y_train.astype(bool),(predicted_training_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "print 'Validation results:'\n",
    "predicted_validation_labels = sess.run(MLP[:,1],feed_dict={MLP_input: X_vali})\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(Y_vali.astype(bool),(predicted_validation_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "print 'Test results:'\n",
    "predicted_test_labels = sess.run(MLP[:,1],feed_dict={MLP_input: X_test})\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(Y_test.astype(bool),(predicted_test_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "fpr, tpr, th = roc_curve(Y_test, predicted_test_labels)\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot2grid((1,2),(0,0))\n",
    "plt.plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid()\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "plt.subplot2grid((1,2),(0,1))\n",
    "plt.plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"False Negative Rate\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.title('DET Curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Random Forest Classification</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "classifier.fit(X_train, Y_train)\n",
    "\n",
    "print 'Training results:'\n",
    "predicted_training_labels = classifier.predict_proba(X_train)[:,1]\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(Y_train.astype(bool),(predicted_training_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "print 'Test results:'\n",
    "predicted_test_labels = classifier.predict_proba(X_test)[:,1]\n",
    "[[VN,FP],[FN,VP]]=confusion_matrix(Y_test.astype(bool),(predicted_test_labels>0.5).astype(bool)).astype(float)\n",
    "print 'VP: %d, VN: %d, FP: %d, FN: %d' %(VP,VN,FP,FN)\n",
    "print 'Porcentaje de clasificaciones correctas: %%%f' %(100.0*(VP+VN)/(VP+VN+FP+FN))\n",
    "print 'Precision: %%%f' %(100.0*VP/(VP+FP))\n",
    "print 'Recall: %%%f' %(100.0*VP/(VP+FN))\n",
    "print ''\n",
    "\n",
    "fpr, tpr, th = roc_curve(Y_test, predicted_test_labels)\n",
    "    \n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot2grid((1,2),(0,0))\n",
    "plt.plot(fpr, tpr, linewidth=2, alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.grid()\n",
    "plt.title('ROC Curve')\n",
    "\n",
    "plt.subplot2grid((1,2),(0,1))\n",
    "plt.plot(fpr, 1.0 - tpr, linewidth=2, alpha=0.5)\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"False Negative Rate\")\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.grid()\n",
    "plt.title('DET Curve')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
